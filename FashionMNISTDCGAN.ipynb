{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "_uuid": "36af9c2edba178f04e989ca2a5935878caf7b0c4",
    "colab": {},
    "colab_type": "code",
    "id": "YfIk2es3hJEd"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\1002983\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished imports\n"
     ]
    }
   ],
   "source": [
    "from __future__ import absolute_import, division, print_function\n",
    "\n",
    "# Import TensorFlow >= 1.10 and enable eager execution\n",
    "import tensorflow as tf\n",
    "tf.enable_eager_execution()\n",
    "\n",
    "import librosa\n",
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "import glob\n",
    "import matplotlib.pyplot as plt\n",
    "import PIL\n",
    "import imageio\n",
    "import pandas as pd\n",
    "import librosa\n",
    "import simpleaudio as sa\n",
    "from IPython import display\n",
    "\n",
    "print(\"Finished imports\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acoustic: 0, electronic: 0, synthesized: 1, total: 1\r",
      "<class 'EagerTensor'>\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "only size-1 arrays can be converted to Python scalars",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-29-25ba813357ad>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     53\u001b[0m                 \u001b[1;34m'velocity'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mFeature\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mint64_list\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mInt64List\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'velocity'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     54\u001b[0m                 \u001b[1;34m'sample_rate'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mFeature\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mint64_list\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mInt64List\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'sample_rate'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 55\u001b[1;33m                 \u001b[1;34m'magnitudes'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mFeature\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfloat_list\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mFloatList\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mmagnitudes\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     56\u001b[0m                 \u001b[1;34m'phases'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mFeature\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfloat_list\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mFloatList\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mphases\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     57\u001b[0m                 \u001b[1;31m#'qualities': tf.train.Feature(int64_list=tf.train.Int64List(value=[data['qualities'].numpy()])),\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\u001b[0m in \u001b[0;36m__repr__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    776\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m__repr__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    777\u001b[0m     return \"<tf.Tensor: id=%s, shape=%s, dtype=%s, numpy=%s>\" % (\n\u001b[1;32m--> 778\u001b[1;33m         self._id, self.shape, self.dtype.name, numpy_text(self, is_repr=True))\n\u001b[0m\u001b[0;32m    779\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    780\u001b[0m   \u001b[1;33m@\u001b[0m\u001b[0mstaticmethod\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\u001b[0m in \u001b[0;36mshape\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    814\u001b[0m       \u001b[1;31m# `_tensor_shape` is declared and defined in the definition of\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    815\u001b[0m       \u001b[1;31m# `EagerTensor`, in C.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 816\u001b[1;33m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_tensor_shape\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtensor_shape\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTensorShape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_shape_tuple\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    817\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_tensor_shape\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    818\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\u001b[0m in \u001b[0;36m__float__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    716\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    717\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m__float__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 718\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mfloat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    719\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    720\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m__array__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: only size-1 arrays can be converted to Python scalars"
     ]
    }
   ],
   "source": [
    "# nsynth = tf.data.Dataset.from_tensor_slices(tf.data.TFRecordDataset(\"nsynth-test.tfrecord\")).shuffle()]\n",
    "nsynth = tf.data.TFRecordDataset(\"nsynth-test.tfrecord\")\n",
    "i = 1\n",
    "numTrue = 0\n",
    "read_features = {\n",
    "    'note': tf.FixedLenFeature([], dtype=tf.int64),\n",
    "    'note_str': tf.FixedLenFeature([], dtype=tf.string),\n",
    "    'instrument': tf.FixedLenFeature([], dtype=tf.int64),\n",
    "    'instrument_str': tf.FixedLenFeature([], dtype=tf.string),\n",
    "    'pitch': tf.FixedLenFeature([], dtype=tf.int64),\n",
    "    'velocity': tf.FixedLenFeature([], dtype=tf.int64),\n",
    "    'sample_rate': tf.FixedLenFeature([], dtype=tf.int64),\n",
    "    'audio': tf.VarLenFeature(dtype=float),\n",
    "    #'qualities': tf.FixedLenFeature(dtype=tf.int64),\n",
    "    #'qualities_str': tf.FixedLenFeature(dtype=tf.string),\n",
    "    'instrument_family': tf.FixedLenFeature([], dtype=tf.int64),\n",
    "    'instrument_family_str': tf.FixedLenFeature([], dtype=tf.string),\n",
    "    'instrument_source': tf.FixedLenFeature([], dtype=tf.int64),\n",
    "    'instrument_source_str': tf.FixedLenFeature([], dtype=tf.string)\n",
    "}\n",
    "#with tf.python_io.TFRecordWriter('spectrograms.tfrecord') as writer:\n",
    "if(True):\n",
    "    acoustic = 0\n",
    "    electronic = 0\n",
    "    synthesized = 0\n",
    "    total = 0\n",
    "    for b in nsynth:\n",
    "        data = tf.parse_single_example(serialized=b, features=read_features)\n",
    "        if(data['instrument_source'].numpy()==0):\n",
    "            acoustic += 1\n",
    "            total += 1\n",
    "        elif(data['instrument_source'].numpy()==1):\n",
    "            electronic += 1\n",
    "            total += 1\n",
    "        elif(data['instrument_source'].numpy()==2):\n",
    "            synthesized += 1\n",
    "            total += 1\n",
    "        print(\"Acoustic: {}, electronic: {}, synthesized: {}, total: {}\".format(acoustic, electronic, synthesized, total), end=\"\\r\")\n",
    "        \n",
    "        #if(data['instrument_source'].numpy()==0):\n",
    "        if(True):\n",
    "            audio = data['audio'].values.numpy()*15000\n",
    "            spectrogram = librosa.stft(audio, 2048)\n",
    "            magnitudes = tf.convert_to_tensor(np.log(np.abs(spectrogram)))\n",
    "            print(type(magnitudes))\n",
    "            phases = tf.convert_to_tensor(np.angle(spectrogram))\n",
    "            example = tf.train.Example(features=tf.train.Features(feature={\n",
    "                'note': tf.train.Feature(int64_list=tf.train.Int64List(value=[data['note'].numpy()])),\n",
    "                'note_str': tf.train.Feature(bytes_list=tf.train.BytesList(value=[data['note_str'].numpy()])),\n",
    "                'instrument': tf.train.Feature(int64_list=tf.train.Int64List(value=[data['instrument'].numpy()])),\n",
    "                'instrument_str': tf.train.Feature(bytes_list=tf.train.BytesList(value=[data['instrument_str'].numpy()])),\n",
    "                'pitch': tf.train.Feature(int64_list=tf.train.Int64List(value=[data['pitch'].numpy()])),\n",
    "                'velocity': tf.train.Feature(int64_list=tf.train.Int64List(value=[data['velocity'].numpy()])),\n",
    "                'sample_rate': tf.train.Feature(int64_list=tf.train.Int64List(value=[data['sample_rate'].numpy()])),\n",
    "                'magnitudes': tf.train.Feature(float_list=tf.train.FloatList(value=[magnitudes])),\n",
    "                'phases': tf.train.Feature(float_list=tf.train.FloatList(value=[phases])),\n",
    "                #'qualities': tf.train.Feature(int64_list=tf.train.Int64List(value=[data['qualities'].numpy()])),\n",
    "                #'qualities_str': tf.train.Feature(bytes_list=tf.train.BytesList(value=[data['qualities_str'].numpy()])),\n",
    "                'instrument_family': tf.train.Feature(int64_list=tf.train.Int64List(value=[data['instrument_family'].numpy()])),\n",
    "                'instrument_family_str': tf.train.Feature(bytes_list=tf.train.BytesList(value=[data['instrument_family_str'].numpy()])),\n",
    "                'instrument_source': tf.train.Feature(int64_list=tf.train.Int64List(value=[data['instrument_source'].numpy()])),\n",
    "                'instrument_source_str': tf.train.Feature(bytes_list=tf.train.BytesList(value=[data['instrument_source_str'].numpy()]))\n",
    "            }))\n",
    "            #writer.write(example.SerializeToString())\n",
    "        break\n",
    "\n",
    "    \"\"\"read_data = tf.parse_single_example(serialized=b, features=read_features)\n",
    "    print(read_data.keys())\n",
    "    print('instrument'+read_data['instrument_str'])\n",
    "    print(read_data['pitch'])\n",
    "    audio = read_data['audio'].values.numpy()*15000\n",
    "    playAudio(audio, read_data['sample_rate'].numpy())\n",
    "    print('First audio playback done')\n",
    "    spectrogram = librosa.stft(audio, 2048)\n",
    "    magnitudes = np.log(np.abs(spectrogram))\n",
    "    phases = np.angle(spectrogram)\n",
    "    plt.imshow(phases, zorder=1, cmap=\"hsv\", origin=\"lower\", aspect=\"auto\")\n",
    "    plt.imshow(magnitudes, zorder=2, alpha=0.9, cmap=\"magma\", origin=\"lower\", aspect=\"auto\")\n",
    "    plt.axis('off')\n",
    "    plt.title('Angles and magnitudes')\n",
    "    plt.figure(3)\n",
    "    plt.plot(audio)\n",
    "    new_spectrogram = np.exp(magnitudes + 1j*phases)\n",
    "    new_audio = librosa.istft(new_spectrogram, 512, 2048)\n",
    "    print(read_data['sample_rate'])\n",
    "    print(type(read_data['sample_rate'].numpy()))\n",
    "    plt.figure(2)\n",
    "    plt.plot(new_audio)\n",
    "    playAudio(new_audio, read_data['sample_rate'].numpy())\n",
    "    print(audio.astype(np.int16)[100:110])\n",
    "    print(new_audio.astype(np.int16)[100:110])\n",
    "    print(np.allclose(audio, new_audio, atol=0.5))\n",
    "    print('Done')\n",
    "    #print(read_data['audio'].reshape((64000,)).eval())\n",
    "    i += 1\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def playAudio(audio, sr):\n",
    "    audio = audio.astype(np.int16)\n",
    "    play_obj = sa.play_buffer(audio, 1, 2, sr)\n",
    "    play_obj.wait_done()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "InvalidArgumentError",
     "evalue": "Key: magnitudes.  Data types don't match. Data type: string but expected type: float [Op:ParseSingleExample]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-17-129fef6a64bd>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[0mdataset\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1025\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m126\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mraw_data\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mspecs\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 21\u001b[1;33m     \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparse_single_example\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mserialized\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mraw_data\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mread_features\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     22\u001b[0m     \u001b[0mmagnitudes\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'magnitudes'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1025\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m126\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m     \u001b[0mphases\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'phases'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1025\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m126\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\parsing_ops.py\u001b[0m in \u001b[0;36mparse_single_example\u001b[1;34m(serialized, features, name, example_names)\u001b[0m\n\u001b[0;32m    756\u001b[0m     \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Missing features.\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    757\u001b[0m   \u001b[1;32mif\u001b[0m \u001b[0mexample_names\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 758\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mparse_single_example_v2\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mserialized\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    759\u001b[0m   \u001b[0mfeatures\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_prepend_none_dimension\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    760\u001b[0m   (sparse_keys, sparse_types, dense_keys, dense_types, dense_defaults,\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\parsing_ops.py\u001b[0m in \u001b[0;36mparse_single_example_v2\u001b[1;34m(serialized, features, name)\u001b[0m\n\u001b[0;32m   1298\u001b[0m   outputs = _parse_single_example_v2_raw(serialized, sparse_keys, sparse_types,\n\u001b[0;32m   1299\u001b[0m                                          \u001b[0mdense_keys\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdense_types\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1300\u001b[1;33m                                          dense_defaults, dense_shapes, name)\n\u001b[0m\u001b[0;32m   1301\u001b[0m   \u001b[1;32mreturn\u001b[0m \u001b[0m_construct_sparse_tensors_for_sparse_features\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1302\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\parsing_ops.py\u001b[0m in \u001b[0;36m_parse_single_example_v2_raw\u001b[1;34m(serialized, sparse_keys, sparse_types, dense_keys, dense_types, dense_defaults, dense_shapes, name)\u001b[0m\n\u001b[0;32m   1415\u001b[0m         \u001b[0mdense_keys\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdense_keys\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1416\u001b[0m         \u001b[0mdense_shapes\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdense_shapes\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1417\u001b[1;33m         name=name)\n\u001b[0m\u001b[0;32m   1418\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1419\u001b[0m     \u001b[1;33m(\u001b[0m\u001b[0msparse_indices\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msparse_values\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msparse_shapes\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdense_values\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0moutputs\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\gen_parsing_ops.py\u001b[0m in \u001b[0;36mparse_single_example\u001b[1;34m(serialized, dense_defaults, num_sparse, sparse_keys, dense_keys, sparse_types, dense_shapes, name)\u001b[0m\n\u001b[0;32m    723\u001b[0m           \u001b[0msparse_keys\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msparse_keys\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdense_keys\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdense_keys\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    724\u001b[0m           \u001b[0msparse_types\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msparse_types\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdense_shapes\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdense_shapes\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 725\u001b[1;33m           ctx=_ctx)\n\u001b[0m\u001b[0;32m    726\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    727\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\gen_parsing_ops.py\u001b[0m in \u001b[0;36mparse_single_example_eager_fallback\u001b[1;34m(serialized, dense_defaults, num_sparse, sparse_keys, dense_keys, sparse_types, dense_shapes, name, ctx)\u001b[0m\n\u001b[0;32m    767\u001b[0m                              \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msparse_types\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mnum_sparse\u001b[0m \u001b[1;33m+\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    768\u001b[0m                              \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdense_defaults\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0m_inputs_flat\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 769\u001b[1;33m                              attrs=_attrs, ctx=_ctx, name=name)\n\u001b[0m\u001b[0;32m    770\u001b[0m   _execute.record_gradient(\n\u001b[0;32m    771\u001b[0m       \"ParseSingleExample\", _inputs_flat, _attrs, _result, name)\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     64\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     65\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 66\u001b[1;33m     \u001b[0msix\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mraise_from\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_status_to_exception\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcode\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     67\u001b[0m   \u001b[1;31m# pylint: enable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     68\u001b[0m   \u001b[1;32mreturn\u001b[0m \u001b[0mtensors\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\six.py\u001b[0m in \u001b[0;36mraise_from\u001b[1;34m(value, from_value)\u001b[0m\n",
      "\u001b[1;31mInvalidArgumentError\u001b[0m: Key: magnitudes.  Data types don't match. Data type: string but expected type: float [Op:ParseSingleExample]"
     ]
    }
   ],
   "source": [
    "read_features = {\n",
    "    'note': tf.FixedLenFeature([], dtype=tf.int64),\n",
    "    'note_str': tf.FixedLenFeature([], dtype=tf.string),\n",
    "    'instrument': tf.FixedLenFeature([], dtype=tf.int64),\n",
    "    'instrument_str': tf.FixedLenFeature([], dtype=tf.string),\n",
    "    'pitch': tf.FixedLenFeature([], dtype=tf.int64),\n",
    "    'velocity': tf.FixedLenFeature([], dtype=tf.int64),\n",
    "    'sample_rate': tf.FixedLenFeature([], dtype=tf.int64),\n",
    "    'magnitudes': tf.FixedLenFeature([1025, 126], dtype=bytes),\n",
    "    'phases': tf.FixedLenFeature([1025, 126], dtype=bytes),\n",
    "    #'qualities': tf.FixedLenFeature(dtype=tf.int64),\n",
    "    #'qualities_str': tf.FixedLenFeature(dtype=tf.string),\n",
    "    'instrument_family': tf.FixedLenFeature([], dtype=tf.int64),\n",
    "    'instrument_family_str': tf.FixedLenFeature([], dtype=tf.string),\n",
    "    'instrument_source': tf.FixedLenFeature([], dtype=tf.int64),\n",
    "    'instrument_source_str': tf.FixedLenFeature([], dtype=tf.string)\n",
    "}\n",
    "specs = tf.data.TFRecordDataset(\"spectrograms.tfrecord\")\n",
    "dataset = tf.zeros([1, 1025, 126, 2])\n",
    "for raw_data in specs:\n",
    "    data = tf.parse_single_example(serialized=raw_data, features=read_features)\n",
    "    magnitudes = tf.reshape(data['magnitudes'], [1, 1025, 126, 1])\n",
    "    phases = tf.reshape(data['phases'], [1, 1025, 126, 1])\n",
    "    spectrogram = tf.concat(magnitudes, phases, 3)\n",
    "    dataset = tf.concat(dataset, spectrogram, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "7400bc7adb624b1580a7663156bdae17502f18b5",
    "colab_type": "text",
    "id": "0TD5ZrvEMbhZ"
   },
   "source": [
    "##### Copyright 2018 The TensorFlow Authors.\n",
    "\n",
    "Licensed under the Apache License, Version 2.0 (the \"License\").\n",
    "\n",
    "# DCGAN: An example with tf.keras and eager\n",
    "\n",
    "<table class=\"tfo-notebook-buttons\" align=\"left\"><td>\n",
    "<a target=\"_blank\"  href=\"https://colab.research.google.com/github/tensorflow/tensorflow/blob/master/tensorflow/contrib/eager/python/examples/generative_examples/dcgan.ipynb\">\n",
    "    <img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\" />Run in Google Colab</a>  \n",
    "</td><td>\n",
    "<a target=\"_blank\"  href=\"https://github.com/tensorflow/tensorflow/tree/master/tensorflow/contrib/eager/python/examples/generative_examples/dcgan.ipynb\"><img width=32px src=\"https://www.tensorflow.org/images/GitHub-Mark-32px.png\" />View source on GitHub</a></td></table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "a9f51581df0d2d9a0168e28f361cc52c5a2f1fbf",
    "colab_type": "text",
    "id": "ITZuApL56Mny"
   },
   "source": [
    "This notebook demonstrates how to generate images of handwritten digits using [tf.keras](https://www.tensorflow.org/programmers_guide/keras) and [eager execution](https://www.tensorflow.org/programmers_guide/eager). To do so, we use Deep Convolutional Generative Adverserial Networks ([DCGAN](https://arxiv.org/pdf/1511.06434.pdf)).\n",
    "\n",
    "This model takes about ~30 seconds per epoch (using tf.contrib.eager.defun to create graph functions) to train on a single Tesla K80 on Colab, as of July 2018.\n",
    "\n",
    "Below is the output generated after training the generator and discriminator models for 150 epochs.\n",
    "\n",
    "![sample output](https://tensorflow.org/images/gan/dcgan.gif)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "54b14c2eb0796084f10136c5668a1be53875fd89",
    "colab": {},
    "colab_type": "code",
    "id": "u_2z-B3piVsw"
   },
   "outputs": [],
   "source": [
    "# to generate gifs\n",
    "!pip install imageio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "71e19069fd6fe6082fd668a4a8752a52682610ee",
    "colab_type": "text",
    "id": "e1_Y75QXJS6h"
   },
   "source": [
    "## Import TensorFlow and enable eager execution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "6835f55bd52acff3ab14f1a54d106940109f68ee",
    "colab_type": "text",
    "id": "iYn4MdZnKCey"
   },
   "source": [
    "## Load the dataset\n",
    "\n",
    "We are going to use the MNIST dataset to train the generator and the discriminator. The generator will then generate handwritten digits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "6c0a173d28b864a3ee734ecbe7aeacde904a3b33",
    "colab": {},
    "colab_type": "code",
    "id": "a4fYMGxGhrna"
   },
   "outputs": [],
   "source": [
    "# Use inside a Kaggle kernel\n",
    "dataset = pd.read_csv(\"../input/fashion-mnist_train.csv\")\n",
    "datasetTest = pd.read_csv(\"../input/fashion-mnist_test.csv\")\n",
    "train_images = dataset[dataset.columns[1:785]].append(datasetTest[datasetTest.columns[1:785]]).values\n",
    "\n",
    "# Use outside of Kaggle\n",
    "# (train_images, train_labels), (_, _) = tf.keras.datasets.mnist.load_data()\n",
    "\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "c644377317890e2e246f469f6acb704c9b315865",
    "colab": {},
    "colab_type": "code",
    "id": "NFC2ghIdiZYE"
   },
   "outputs": [],
   "source": [
    "train_images = tf.constant(train_images, shape = [70000, 784])\n",
    "train_images = tf.cast(tf.reshape(train_images, (train_images.shape[0], 28, 28, 1)),'float32')\n",
    "# We are normalizing the images to the range of [-1, 1]\n",
    "train_images = (train_images - 127.5) / 127.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "c6cfcdfef1caf6dd8cd6e197105d47f314b6452a",
    "colab": {},
    "colab_type": "code",
    "id": "S4PIDhoDLbsZ"
   },
   "outputs": [],
   "source": [
    "BUFFER_SIZE = 70000\n",
    "BATCH_SIZE = 256"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "11aaa1d18353a61357fd610bd0362a3f68f241d7",
    "colab_type": "text",
    "id": "PIGN6ouoQxt3"
   },
   "source": [
    "## Use tf.data to create batches and shuffle the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "a507a38205e59cb2056cf1e518e39021bfbec985",
    "colab": {},
    "colab_type": "code",
    "id": "-yKCCQOoJ7cn"
   },
   "outputs": [],
   "source": [
    "train_dataset = tf.data.Dataset.from_tensor_slices(train_images).shuffle(BUFFER_SIZE).batch(BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "0b37e5e63b1cc242f5b7cb8988bcbf4a1155d280",
    "colab_type": "text",
    "id": "THY-sZMiQ4UV"
   },
   "source": [
    "## Write the generator and discriminator models\n",
    "\n",
    "* **Generator** \n",
    "  * It is responsible for **creating convincing images that are good enough to fool the discriminator**.\n",
    "  * It consists of Conv2DTranspose (Upsampling) layers. We start with a fully connected layer and upsample the image 2 times so as to reach the desired image size (mnist image size) which is (28, 28, 1). \n",
    "  * We use **leaky relu** activation except for the **last layer** which uses **tanh** activation.\n",
    "  \n",
    "* **Discriminator**\n",
    "  * **The discriminator is responsible for classifying the fake images from the real images.**\n",
    "  * In other words, the discriminator is given generated images (from the generator) and the real MNIST images. The job of the discriminator is to classify these images into fake (generated) and real (MNIST images).\n",
    "  * **Basically the generator should be good enough to fool the discriminator that the generated images are real**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "f50d15ea77525e355d3b1cca08240a3d920fec3a",
    "colab": {},
    "colab_type": "code",
    "id": "VGLbvBEmjK0a"
   },
   "outputs": [],
   "source": [
    "class Generator(tf.keras.Model):\n",
    "    def __init__(self):\n",
    "        super(Generator, self).__init__()\n",
    "        self.fc1 = tf.keras.layers.Dense(7*7*64, use_bias=False)\n",
    "        self.batchnorm1 = tf.keras.layers.BatchNormalization()\n",
    "    \n",
    "        self.conv1 = tf.keras.layers.Conv2DTranspose(64, (5, 5), strides=(1, 1), padding='same', use_bias=False)\n",
    "        self.batchnorm2 = tf.keras.layers.BatchNormalization()\n",
    "    \n",
    "        self.conv2 = tf.keras.layers.Conv2DTranspose(32, (5, 5), strides=(2, 2), padding='same', use_bias=False)\n",
    "        self.batchnorm3 = tf.keras.layers.BatchNormalization()\n",
    "    \n",
    "        self.conv3 = tf.keras.layers.Conv2DTranspose(1, (5, 5), strides=(2, 2), padding='same', use_bias=False)\n",
    "\n",
    "    def call(self, x, training=True):\n",
    "        x = self.fc1(x)\n",
    "        x = self.batchnorm1(x, training=training)\n",
    "        x = tf.nn.relu(x)\n",
    "\n",
    "        x = tf.reshape(x, shape=(-1, 7, 7, 64))\n",
    "\n",
    "        x = self.conv1(x)\n",
    "        x = self.batchnorm2(x, training=training)\n",
    "        x = tf.nn.relu(x)\n",
    "\n",
    "        x = self.conv2(x)\n",
    "        x = self.batchnorm3(x, training=training)\n",
    "        x = tf.nn.relu(x)\n",
    "\n",
    "        x = tf.nn.tanh(self.conv3(x))  \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "92507e1426e5d24567fc5ff7e982a512826046f6",
    "colab": {},
    "colab_type": "code",
    "id": "bkOfJxk5j5Hi"
   },
   "outputs": [],
   "source": [
    "class Discriminator(tf.keras.Model):\n",
    "    def __init__(self):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.conv1 = tf.keras.layers.Conv2D(64, (5, 5), strides=(2, 2), padding='same')\n",
    "        self.conv2 = tf.keras.layers.Conv2D(128, (5, 5), strides=(2, 2), padding='same')\n",
    "        self.dropout = tf.keras.layers.Dropout(0.3)\n",
    "        self.flatten = tf.keras.layers.Flatten()\n",
    "        self.fc1 = tf.keras.layers.Dense(1)\n",
    "\n",
    "    def call(self, x, training=True):\n",
    "        x = tf.nn.leaky_relu(self.conv1(x))\n",
    "        x = self.dropout(x, training=training)\n",
    "        x = tf.nn.leaky_relu(self.conv2(x))\n",
    "        x = self.dropout(x, training=training)\n",
    "        x = self.flatten(x)\n",
    "        x = self.fc1(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "27be98d90b697245cc674259d60043903d127a1e",
    "colab": {},
    "colab_type": "code",
    "id": "gDkA05NE6QMs"
   },
   "outputs": [],
   "source": [
    "generator = Generator()\n",
    "discriminator = Discriminator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "79beb4e62d0b0cefd6b06d24d481c6b0f3f846d2",
    "colab": {},
    "colab_type": "code",
    "id": "k1HpMSLImuRi"
   },
   "outputs": [],
   "source": [
    "# Defun gives 10 secs/epoch performance boost\n",
    "generator.call = tf.contrib.eager.defun(generator.call)\n",
    "discriminator.call = tf.contrib.eager.defun(discriminator.call)\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "546fbaed647277db33477433fab8de5e09b8507c",
    "colab_type": "text",
    "id": "0FMYgY_mPfTi"
   },
   "source": [
    "## Define the loss functions and the optimizer\n",
    "\n",
    "* **Discriminator loss**\n",
    "  * The discriminator loss function takes 2 inputs; **real images, generated images**\n",
    "  * real_loss is a sigmoid cross entropy loss of the **real images** and an **array of ones (since these are the real images)**\n",
    "  * generated_loss is a sigmoid cross entropy loss of the **generated images** and an **array of zeros (since these are the fake images)**\n",
    "  * Then the total_loss is the sum of real_loss and the generated_loss\n",
    "  \n",
    "* **Generator loss**\n",
    "  * It is a sigmoid cross entropy loss of the generated images and an **array of ones**\n",
    "  \n",
    "\n",
    "* The discriminator and the generator optimizers are different since we will train them separately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "47ae70cfe471b80ac827a5772bcba07ec890450f",
    "colab": {},
    "colab_type": "code",
    "id": "wkMNfBWlT-PV"
   },
   "outputs": [],
   "source": [
    "def discriminator_loss(real_output, generated_output):\n",
    "    # [1,1,...,1] with real output since it is true and we want\n",
    "    # our generated examples to look like it\n",
    "    real_loss = tf.losses.sigmoid_cross_entropy(multi_class_labels=tf.ones_like(real_output), logits=real_output)\n",
    "\n",
    "    # [0,0,...,0] with generated images since they are fake\n",
    "    generated_loss = tf.losses.sigmoid_cross_entropy(multi_class_labels=tf.zeros_like(generated_output), logits=generated_output)\n",
    "\n",
    "    total_loss = real_loss + generated_loss\n",
    "\n",
    "    return total_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "df009f9802ad5e4ecca74e113a4501340029bba3",
    "colab": {},
    "colab_type": "code",
    "id": "90BIcCKcDMxz"
   },
   "outputs": [],
   "source": [
    "def generator_loss(generated_output):\n",
    "    return tf.losses.sigmoid_cross_entropy(tf.ones_like(generated_output), generated_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "b3e842f37d4db87b83d2612eead6b0d0315f0dd7",
    "colab": {},
    "colab_type": "code",
    "id": "iWCn_PVdEJZ7"
   },
   "outputs": [],
   "source": [
    "discriminator_optimizer = tf.train.AdamOptimizer(1e-4)\n",
    "generator_optimizer = tf.train.AdamOptimizer(1e-4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "6b18f2143da7577f6bdc515c26ce48fa1b7e3c91",
    "colab_type": "text",
    "id": "mWtinsGDPJlV"
   },
   "source": [
    "## Checkpoints (Object-based saving)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "b3d909e947bc7fcba6dd2b2851f67396176cdbc2",
    "colab": {},
    "colab_type": "code",
    "id": "CA1w-7s2POEy"
   },
   "outputs": [],
   "source": [
    "# checkpoint_dir = './training_checkpoints'\n",
    "# checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")\n",
    "# checkpoint = tf.train.Checkpoint(generator_optimizer=generator_optimizer,\n",
    "#                                  discriminator_optimizer=discriminator_optimizer,\n",
    "#                                  generator=generator,\n",
    "#                                  discriminator=discriminator)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "3bc7f14b0173073ee003c2f6e8c3b1a3ee5d06ce",
    "colab_type": "text",
    "id": "Rw1fkAczTQYh"
   },
   "source": [
    "## Training\n",
    "\n",
    "* We start by iterating over the dataset\n",
    "* The generator is given **noise as an input** which when passed through the generator model will output a image looking like a handwritten digit\n",
    "* The discriminator is given the **real MNIST images as well as the generated images (from the generator)**.\n",
    "* Next, we calculate the generator and the discriminator loss.\n",
    "* Then, we calculate the gradients of loss with respect to both the generator and the discriminator variables (inputs) and apply those to the optimizer.\n",
    "\n",
    "## Generate Images\n",
    "\n",
    "* After training, its time to generate some images!\n",
    "* We start by creating noise array as an input to the generator\n",
    "* The generator will then convert the noise into handwritten images.\n",
    "* Last step is to plot the predictions and **voila!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "cbe0b356b41cbdeb6751dfde003e8590cb68586b",
    "colab": {},
    "colab_type": "code",
    "id": "NS2GWywBbAWo"
   },
   "outputs": [],
   "source": [
    "EPOCHS = 150\n",
    "noise_dim = 100\n",
    "num_examples_to_generate = 16\n",
    "\n",
    "# keeping the random vector constant for generation (prediction) so\n",
    "# it will be easier to see the improvement of the gan.\n",
    "random_vector_for_generation = tf.random_normal([num_examples_to_generate,\n",
    "                                                 noise_dim])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "16478afa6311040cd8132a2c1980652a656618e6",
    "colab": {},
    "colab_type": "code",
    "id": "RmdVsmvhPxyy"
   },
   "outputs": [],
   "source": [
    "def generate_and_save_images(model, epoch, test_input):\n",
    "    # make sure the training parameter is set to False because we\n",
    "    # don't want to train the batchnorm layer when doing inference.\n",
    "    predictions = model(test_input, training=False)\n",
    "\n",
    "    fig = plt.figure(figsize=(4,4))\n",
    "\n",
    "    for i in range(predictions.shape[0]):\n",
    "        plt.subplot(4, 4, i+1)\n",
    "        plt.imshow(predictions[i, :, :, 0] * 127.5 + 127.5, cmap='gray')\n",
    "        plt.axis('off')\n",
    "\n",
    "    plt.savefig('image_at_epoch_{:04d}.png'.format(epoch))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "ac0e304af171a8d292728d743ae39e464da065cf",
    "colab": {},
    "colab_type": "code",
    "id": "2M7LmLtGEMQJ"
   },
   "outputs": [],
   "source": [
    "def train(dataset, epochs, noise_dim):  \n",
    "    for epoch in range(epochs):\n",
    "        start = time.time()\n",
    "    \n",
    "        for images in dataset:\n",
    "            # generating noise from a uniform distribution\n",
    "            noise = tf.random_normal([BATCH_SIZE, noise_dim])\n",
    "\n",
    "            with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:\n",
    "                generated_images = generator(noise, training=True)\n",
    "\n",
    "                real_output = discriminator(images, training=True)\n",
    "                generated_output = discriminator(generated_images, training=True)\n",
    "\n",
    "                gen_loss = generator_loss(generated_output)\n",
    "                disc_loss = discriminator_loss(real_output, generated_output)\n",
    "\n",
    "            gradients_of_generator = gen_tape.gradient(gen_loss, generator.variables)\n",
    "            gradients_of_discriminator = disc_tape.gradient(disc_loss, discriminator.variables)\n",
    "\n",
    "            generator_optimizer.apply_gradients(zip(gradients_of_generator, generator.variables))\n",
    "            discriminator_optimizer.apply_gradients(zip(gradients_of_discriminator, discriminator.variables))\n",
    "\n",
    "        if epoch % 1 == 0:\n",
    "            display.clear_output(wait=True)\n",
    "            generate_and_save_images(generator,\n",
    "                                   epoch + 1,\n",
    "                                   random_vector_for_generation)\n",
    "\n",
    "        # saving (checkpoint) the model every 15 epochs\n",
    "        #if (epoch + 1) % 15 == 0:\n",
    "            #checkpoint.save(file_prefix = checkpoint_prefix)\n",
    "\n",
    "        print ('Time taken for epoch {} is {} sec'.format(epoch + 1,\n",
    "                                                          time.time()-start))\n",
    "  # generating after the final epoch\n",
    "    display.clear_output(wait=True)\n",
    "    generate_and_save_images(generator,\n",
    "                           epochs,\n",
    "                           random_vector_for_generation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-output": true,
    "_uuid": "446c1e4d2462906cc5ed888104e549ac86b476b2",
    "colab": {},
    "colab_type": "code",
    "id": "Ly3UN0SLLY2l",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print(\"Start training\")\n",
    "train(train_dataset, EPOCHS, noise_dim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "9e5c596190985b8954f9ba6bec73bc504a0f9465",
    "colab_type": "text",
    "id": "rfM4YcPVPkNO"
   },
   "source": [
    "## Restore the latest checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "0476d178527341a5d84219855f70f44a161f5c7c",
    "colab": {},
    "colab_type": "code",
    "id": "XhXsd0srPo8c"
   },
   "outputs": [],
   "source": [
    "# restoring the latest checkpoint in checkpoint_dir\n",
    "# checkpoint.restore(tf.train.latest_checkpoint(checkpoint_dir))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "601fa61b7f04044bc352d8d13f9dfce246080614",
    "colab_type": "text",
    "id": "P4M_vIbUi7c0"
   },
   "source": [
    "## Display an image using the epoch number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "3d21c33fa4fecb4c72ed49e0ae0b9f6abee75dba",
    "colab": {},
    "colab_type": "code",
    "id": "WfO5wCdclHGL"
   },
   "outputs": [],
   "source": [
    "# def display_image(epoch_no):\n",
    "#     return PIL.Image.open('image_at_epoch_{:04d}.png'.format(epoch_no))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "0232942a4f22dedcecae086388e4722fb4906ace",
    "colab": {},
    "colab_type": "code",
    "id": "5x3q9_Oe5q0A"
   },
   "outputs": [],
   "source": [
    "# display_image(EPOCHS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "198958796c1208b11522c22e4742eb1820a35389",
    "colab_type": "text",
    "id": "NywiH3nL8guF"
   },
   "source": [
    "## Generate a GIF of all the saved images."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "11fdb14d3158b74c7f5676863c9d81fb1447756e",
    "colab_type": "text",
    "id": "xmO0Dmu2WICn"
   },
   "source": [
    "<!-- TODO(markdaoust): Remove the hack when Ipython version is updated -->\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "18147fa7ac84aa1a5ffdb980525af8fd56583427",
    "colab": {},
    "colab_type": "code",
    "id": "IGKQgENQ8lEI"
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "with imageio.get_writer('dcgan.gif', mode='I') as writer:\n",
    "    filenames = glob.glob('image*.png')\n",
    "    filenames = sorted(filenames)\n",
    "    last = -1\n",
    "    for i,filename in enumerate(filenames):\n",
    "        frame = 2*(i**0.5)\n",
    "        if round(frame) > round(last):\n",
    "            last = frame\n",
    "        else:\n",
    "            continue\n",
    "        image = imageio.imread(filename)\n",
    "        writer.append_data(image)\n",
    "    image = imageio.imread(filename)\n",
    "    writer.append_data(image)\n",
    "    \n",
    "# this is a hack to display the gif inside the notebook\n",
    "os.system('cp dcgan.gif dcgan.gif.png')\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "f4c2daa752c2ee02dfea5ad8355b36461f438fb8",
    "colab": {},
    "colab_type": "code",
    "id": "uV0yiKpzNP1b"
   },
   "outputs": [],
   "source": [
    "# display.Image(filename=\"dcgan.gif.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "0ad474f94184ff9fffeb10188b3c5024b7b9edb2",
    "colab_type": "text",
    "id": "6EEG-wePkmJQ"
   },
   "source": [
    "To downlod the animation from Colab uncomment the code below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "a108d7fabd182f785c8adc45e33c5dd97bac348b",
    "colab": {},
    "colab_type": "code",
    "id": "4UJjSnIMOzOJ"
   },
   "outputs": [],
   "source": [
    "#from google.colab import files\n",
    "#files.download('dcgan.gif')"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "dcgan.ipynb",
   "private_outputs": true,
   "provenance": [
    {
     "file_id": "1eb0NOTQapkYs3X0v-zL1x5_LFKgDISnp",
     "timestamp": 1527173385672
    }
   ],
   "toc_visible": true,
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
